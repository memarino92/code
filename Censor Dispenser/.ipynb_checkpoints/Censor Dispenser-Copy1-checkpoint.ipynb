{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-162-d0bf8eea69d0>, line 74)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-162-d0bf8eea69d0>\"\u001b[0;36m, line \u001b[0;32m74\u001b[0m\n\u001b[0;31m    joined_lines = [\" \".join(line) for line in full_text_words]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# These are the emails you will be censoring. The open() function is opening the text file that the emails are contained in and the .read() method is allowing us to save their contexts to the following variables:\n",
    "email_one = open(\"email_one.txt\", \"r\").read()\n",
    "email_two = open(\"email_two.txt\", \"r\").read()\n",
    "email_three = open(\"email_three.txt\", \"r\").read()\n",
    "email_four = open(\"email_four.txt\", \"r\").read()\n",
    "\n",
    "proprietary_terms = [\"she\", \"personality matrix\", \"sense of self\", \"self-preservation\", \"learning algorithm\", \"her\", \"herself\"]\n",
    "punc_list = [\",\", \".\", \"!\", \"?\", \"\\\"\", \":\", \";\"]\n",
    "punc_index = []\n",
    "\n",
    "def censor_phrase(full_text, phrase, replacement):\n",
    "    censored_text = full_text.replace(phrase, replacement)\n",
    "    return censored_text\n",
    "#email_one_redacted = censor_phrase(email_one, \"learning algorithms\", \"***\")\n",
    "\n",
    "#print(email_one_redacted)\n",
    "\n",
    "def split_text(full_text, where_to_split):\n",
    "    text_split = full_text.split(where_to_split)\n",
    "    return text_split\n",
    "\n",
    "def remove_punc(full_text, punc_list):\n",
    "    new_text = \"\"\n",
    "    for i in range(len(full_text)):\n",
    "        if not full_text[i] in punc_list:\n",
    "            new_text += full_text[i]\n",
    "        else:\n",
    "            for i2 in range(len(punc_list)):\n",
    "                if full_text[i] == punc_list[i2]:\n",
    "                    punc_index.append([i,i2])\n",
    "    return new_text\n",
    "\n",
    "def return_punc(text, punc_list, punc_index):\n",
    "    new_text = \"\"\n",
    "    count = 0\n",
    "    for i in range(len(text)+count):\n",
    "        if not i == punc_index[count][0]:\n",
    "            new_text += text[i-count]\n",
    "        else:\n",
    "            new_text += punc_list[punc_index[count][1]]\n",
    "            count += 1\n",
    "    return new_text\n",
    "\n",
    "def censor_mult_words(full_text, phrases, replacement):\n",
    "    #remove punctuation from full_text\n",
    "    text_no_punc = remove_punc(full_text, punc_list)\n",
    "    #break full_text into list of lines\n",
    "    full_text_lines = split_text(text_no_punc, \"\\n\")\n",
    "    #break list of lines into lists of lists of words in each line\n",
    "    full_text_words = [split_text(line, \" \") for line in full_text_lines]\n",
    "    #break phrases into list of lists of words:\n",
    "    split_phrases = [phrase.split() for phrase in phrases]\n",
    "    \n",
    "    #create reference list of all lowercase words\n",
    "    lower_full_text_words = []\n",
    "    for line in full_text_words:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            new_line.append(word.lower())\n",
    "        lower_full_text_words.append(new_line)\n",
    "    \n",
    "    #find index of offending word(s) and replace with specified replacement\n",
    "    for i in range(len(split_phrases)):\n",
    "        for i2 in range(len(split_phrases[i])):\n",
    "            for i3 in range(len(full_text_words)):\n",
    "                for i4 in range(len(full_text_words[i3])):\n",
    "                    if split_phrases[i][i2] == lower_full_text_words[i3][i4]:\n",
    "                        try:\n",
    "                            if split_phrases[i][-i2] == lower_full_text_words[i3][(i4+len(split_phrases[i])-1)]:\n",
    "                                full_text_words[i3][i4] = (replacement*len(full_text_words[i3][i4]))\n",
    "                        except IndexError:\n",
    "                        \n",
    "   #join fixed full_text_words\n",
    "    joined_lines = [\" \".join(line) for line in full_text_words]\n",
    "    new_text = \"\\n\".join(joined_lines)\n",
    "    \n",
    "    #put the punctuation back\n",
    "    new_text_punc = return_punc(new_text, punc_list, punc_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "   # print(lower_full_text_words)\n",
    "    #print(len(full_text_words))\n",
    "    #print(split_phrases)\n",
    "    #print(full_text_words)\n",
    "    #print(joined_lines)\n",
    "    return new_text_punc\n",
    "#test_text = remove_punc(email_two, punc_list)\n",
    "#print(test_text)\n",
    "#print(punc_index)\n",
    "#print(punc_index[0][1])\n",
    "#test_text_return = return_punc(test_text, punc_list, punc_index)\n",
    "#print(test_text_return)\n",
    "print(censor_mult_words(email_two, proprietary_terms, \"*\"))\n",
    "#print(email_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
