{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are the emails you will be censoring. The open() function is opening the text file that the emails are contained in and the .read() method is allowing us to save their contexts to the following variables:\n",
    "email_one = open(\"email_one.txt\", \"r\").read()\n",
    "email_two = open(\"email_two.txt\", \"r\").read()\n",
    "email_three = open(\"email_three.txt\", \"r\").read()\n",
    "email_four = open(\"email_four.txt\", \"r\").read()\n",
    "\n",
    "proprietary_terms = [\"she\", \"personality matrix\", \"sense of self\", \"self-preservation\", \"learning algorithms\", \"her\", \"herself\"]\n",
    "negative_words = [\"concerned\", \"behind\", \"danger\", \"dangerous\", \"alarming\", \"alarmed\", \"out of control\", \"help\", \"unhappy\", \"bad\", \"upset\", \"awful\", \"broken\", \"damage\", \"damaging\", \"dismal\", \"distressed\", \"distressing\", \"concerning\", \"horrible\", \"horribly\", \"questionable\"]\n",
    "punc_list = [\",\", \".\", \"!\", \"?\", \"\\\"\", \":\", \";\", \"(\", \")\"]\n",
    "punc_index = []\n",
    "\n",
    "def censor_phrase(full_text, phrase, replacement):\n",
    "    censored_text = full_text.replace(phrase, replacement)\n",
    "    return censored_text\n",
    "#email_one_redacted = censor_phrase(email_one, \"learning algorithms\", \"***\")\n",
    "\n",
    "#print(email_one_redacted)\n",
    "\n",
    "def split_text(full_text, where_to_split):\n",
    "    text_split = full_text.split(where_to_split)\n",
    "    return text_split\n",
    "\n",
    "def remove_punc(full_text, punc_list):\n",
    "    new_text = \"\"\n",
    "    for i in range(len(full_text)):\n",
    "        if not full_text[i] in punc_list:\n",
    "            new_text += full_text[i]\n",
    "        else:\n",
    "            for i2 in range(len(punc_list)):\n",
    "                if full_text[i] == punc_list[i2]:\n",
    "                    punc_index.append([i,i2])\n",
    "    return new_text\n",
    "\n",
    "def return_punc(text, punc_list, punc_index):\n",
    "    new_text = \"\"\n",
    "    count = 0\n",
    "    for i in range(len(text)+count):\n",
    "        if not i == punc_index[count][0]:\n",
    "            new_text += text[i-count]\n",
    "        else:\n",
    "            new_text += punc_list[punc_index[count][1]]\n",
    "            count += 1\n",
    "    return new_text\n",
    "\n",
    "def censor_mult_words(full_text, phrases, replacement):\n",
    "    #remove punctuation from full_text\n",
    "    text_no_punc = remove_punc(full_text, punc_list)\n",
    "    #break full_text into list of lines\n",
    "    full_text_lines = split_text(text_no_punc, \"\\n\")\n",
    "    #break list of lines into lists of lists of words in each line\n",
    "    full_text_words = [split_text(line, \" \") for line in full_text_lines]\n",
    "    #break phrases into list of lists of words:\n",
    "    split_phrases = [phrase.split() for phrase in phrases]\n",
    "    \n",
    "    #create reference list of all lowercase words\n",
    "    lower_full_text_words = []\n",
    "    for line in full_text_words:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            new_line.append(word.lower())\n",
    "        lower_full_text_words.append(new_line)\n",
    "    \n",
    "    #find index of offending word(s) and replace with specified replacement\n",
    "    for i in range(len(split_phrases)):\n",
    "        #for i2 in range(len(split_phrases[i])):\n",
    "            for i3 in range(len(full_text_words)):\n",
    "                for i4 in range(len(full_text_words[i3])):\n",
    "                    if split_phrases[i][0:] == lower_full_text_words[i3][i4:i4+(len(split_phrases[i]))]:\n",
    "                        for i2 in range(len(split_phrases[i])):\n",
    "                            full_text_words[i3][i2+i4] = replacement*len(lower_full_text_words[i3][i2+i4])\n",
    "\n",
    "   #join fixed full_text_words\n",
    "    joined_lines = [\" \".join(line) for line in full_text_words]\n",
    "    new_text = \"\\n\".join(joined_lines)\n",
    "    \n",
    "    #put the punctuation back\n",
    "    new_text_punc = return_punc(new_text, punc_list, punc_index)\n",
    "    \n",
    "    return new_text_punc\n",
    "\n",
    "#print(censor_mult_words(email_two, proprietary_terms, \"*\"))\n",
    "\n",
    "def censor_mult_and_neg(full_text, phrases, negative_words, replacement):\n",
    "    #remove punctuation from full_text\n",
    "    text_no_punc = remove_punc(full_text, punc_list)\n",
    "    #break full_text into list of lines\n",
    "    full_text_lines = split_text(text_no_punc, \"\\n\")\n",
    "    #break list of lines into lists of lists of words in each line\n",
    "    full_text_words = [split_text(line, \" \") for line in full_text_lines]\n",
    "    #break phrases into list of lists of words:\n",
    "    split_phrases = [phrase.split() for phrase in phrases]\n",
    "    \n",
    "    #create reference list of all lowercase words\n",
    "    lower_full_text_words = []\n",
    "    for line in full_text_words:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            new_line.append(word.lower())\n",
    "        lower_full_text_words.append(new_line)\n",
    "    \n",
    "    #find index of offending word(s) and replace with specified replacement\n",
    "    for i in range(len(split_phrases)):\n",
    "        for i3 in range(len(full_text_words)):\n",
    "            for i4 in range(len(full_text_words[i3])):\n",
    "                if split_phrases[i][0:] == lower_full_text_words[i3][i4:i4+(len(split_phrases[i]))]:\n",
    "                    for i2 in range(len(split_phrases[i])):\n",
    "                        full_text_words[i3][i2+i4] = replacement*len(lower_full_text_words[i3][i2+i4])\n",
    "    \n",
    "    #negative words counter and censor\n",
    "    neg_split = [neg_word.split() for neg_word in negative_words]\n",
    "    neg_word_count = 0\n",
    "    \n",
    "    for i3 in range(len(full_text_words)):\n",
    "        for i4 in range(len(full_text_words[i3])):\n",
    "            for i in range(len(neg_split)):\n",
    "                if neg_split[i][0:] == lower_full_text_words[i3][i4:i4+(len(neg_split[i]))]:\n",
    "                    print(neg_split[i][0:])\n",
    "                    neg_word_count += 1\n",
    "                    print(neg_word_count)\n",
    "                    if neg_word_count > 2:\n",
    "                        for i2 in range(len(neg_split[i])):\n",
    "                            full_text_words[i3][i2+i4] = replacement*len(full_text_words[i3][i2+i4])\n",
    "    \n",
    "                            \n",
    "    #print(neg_split)                       \n",
    "   #join fixed full_text_words\n",
    "    joined_lines = [\" \".join(line) for line in full_text_words]\n",
    "    new_text = \"\\n\".join(joined_lines)\n",
    "    \n",
    "    #put the punctuation back\n",
    "    new_text_punc = return_punc(new_text, punc_list, punc_index)\n",
    "    \n",
    "    return new_text_punc\n",
    "#print(email_three)\n",
    "#print(censor_mult_and_neg(email_three, proprietary_terms, negative_words, \"*\"))\n",
    "\n",
    "def censor_it_all(full_text, phrases, negative_words, replacement):\n",
    "    #remove punctuation from full_text\n",
    "    text_no_punc = remove_punc(full_text, punc_list)\n",
    "    #break full_text into list of lines\n",
    "    full_text_lines = split_text(text_no_punc, \"\\n\")\n",
    "    #break list of lines into lists of lists of words in each line\n",
    "    full_text_words = [split_text(line, \" \") for line in full_text_lines]\n",
    "    #break phrases into list of lists of words:\n",
    "    split_phrases = [phrase.split() for phrase in phrases]\n",
    "    #create list of indexes of words that have been changed\n",
    "    words_censored = []\n",
    "    #create reference list of all lowercase words\n",
    "    lower_full_text_words = []\n",
    "    for line in full_text_words:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            new_line.append(word.lower())\n",
    "        lower_full_text_words.append(new_line)\n",
    "    \n",
    "    #find index of offending word(s) and replace with specified replacement\n",
    "    for i in range(len(split_phrases)):\n",
    "        for i3 in range(len(full_text_words)):\n",
    "            for i4 in range(len(full_text_words[i3])):\n",
    "                if split_phrases[i][0:] == lower_full_text_words[i3][i4:i4+(len(split_phrases[i]))]:\n",
    "                    for i2 in range(len(split_phrases[i])):\n",
    "                        full_text_words[i3][i2+i4] = replacement*len(lower_full_text_words[i3][i2+i4])\n",
    "                        words_censored.append([i3, i2+i4])\n",
    "    #negative words counter and censor\n",
    "    neg_split = [neg_word.split() for neg_word in negative_words]\n",
    "    neg_word_count = 0\n",
    "    \n",
    "    for i3 in range(len(full_text_words)):\n",
    "        for i4 in range(len(full_text_words[i3])):\n",
    "            for i in range(len(neg_split)):\n",
    "                if neg_split[i][0:] == lower_full_text_words[i3][i4:i4+(len(neg_split[i]))]:\n",
    "                    for i2 in range(len(neg_split[i])):\n",
    "                        full_text_words[i3][i2+i4] = replacement*len(full_text_words[i3][i2+i4])\n",
    "                        words_censored.append([i3, i2+i4])\n",
    "    #censor surrounding words\n",
    "    for i in words_censored:\n",
    "        try:\n",
    "            full_text_words[i[0]][i[1]+1] = replacement*len(full_text_words[i[0]][i[1]+1])\n",
    "            full_text_words[i[0]][i[1]-1] = replacement*len(full_text_words[i[0]][i[1]-1])\n",
    "        except IndexError:\n",
    "            continue\n",
    "    print(words_censored)                       \n",
    "   #join fixed full_text_words\n",
    "    joined_lines = [\" \".join(line) for line in full_text_words]\n",
    "    new_text = \"\\n\".join(joined_lines)\n",
    "    \n",
    "    #put the punctuation back\n",
    "    new_text_punc = return_punc(new_text, punc_list, punc_index)\n",
    "    \n",
    "    return new_text_punc\n",
    "#print(censor_it_all(email_four, proprietary_terms, negative_words, \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
