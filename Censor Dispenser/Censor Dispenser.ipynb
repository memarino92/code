{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Morning, Board of Investors,\n",
      "\n",
      "Lots of updates this week. The ******** ********** have been working better than we could have ever expected. Our initial internal data dumps have been completed and we have proceeded with the plan to connect the system to the internet and wow! The results are mind blowing.\n",
      "\n",
      "*** is learning faster than ever. *** learning rate now that *** has access to the world wide web has increased exponentially, far faster than we had though the ******** ********** were capable of. \n",
      "\n",
      "Not only that, but we have configured *** *********** ****** to allow for communication between the system and our team of researchers. That's how we know *** considers ******* to be a ***! We asked!\n",
      "\n",
      "How cool is that? We didn't expect a personality to develop this early on in the process but it seems like a rudimentary ***** ** **** is starting to form. This is a major step in the process, as having a ***** ** **** and ***************** will allow *** to see the problems the world is facing and make hard but necessary decisions for the betterment of the planet.\n",
      "\n",
      "We are a-buzz down in the lab with excitement over these developments and we hope that the investors share our enthusiasm.\n",
      "\n",
      "Till next month,\n",
      "Fran\n"
     ]
    }
   ],
   "source": [
    "# These are the emails you will be censoring. The open() function is opening the text file that the emails are contained in and the .read() method is allowing us to save their contexts to the following variables:\n",
    "email_one = open(\"email_one.txt\", \"r\").read()\n",
    "email_two = open(\"email_two.txt\", \"r\").read()\n",
    "email_three = open(\"email_three.txt\", \"r\").read()\n",
    "email_four = open(\"email_four.txt\", \"r\").read()\n",
    "\n",
    "proprietary_terms = [\"she\", \"personality matrix\", \"sense of self\", \"self-preservation\", \"learning algorithms\", \"her\", \"herself\"]\n",
    "punc_list = [\",\", \".\", \"!\", \"?\", \"\\\"\", \":\", \";\"]\n",
    "punc_index = []\n",
    "\n",
    "def censor_phrase(full_text, phrase, replacement):\n",
    "    censored_text = full_text.replace(phrase, replacement)\n",
    "    return censored_text\n",
    "#email_one_redacted = censor_phrase(email_one, \"learning algorithms\", \"***\")\n",
    "\n",
    "#print(email_one_redacted)\n",
    "\n",
    "def split_text(full_text, where_to_split):\n",
    "    text_split = full_text.split(where_to_split)\n",
    "    return text_split\n",
    "\n",
    "def remove_punc(full_text, punc_list):\n",
    "    new_text = \"\"\n",
    "    for i in range(len(full_text)):\n",
    "        if not full_text[i] in punc_list:\n",
    "            new_text += full_text[i]\n",
    "        else:\n",
    "            for i2 in range(len(punc_list)):\n",
    "                if full_text[i] == punc_list[i2]:\n",
    "                    punc_index.append([i,i2])\n",
    "    return new_text\n",
    "\n",
    "def return_punc(text, punc_list, punc_index):\n",
    "    new_text = \"\"\n",
    "    count = 0\n",
    "    for i in range(len(text)+count):\n",
    "        if not i == punc_index[count][0]:\n",
    "            new_text += text[i-count]\n",
    "        else:\n",
    "            new_text += punc_list[punc_index[count][1]]\n",
    "            count += 1\n",
    "    return new_text\n",
    "\n",
    "def censor_mult_words(full_text, phrases, replacement):\n",
    "    #remove punctuation from full_text\n",
    "    text_no_punc = remove_punc(full_text, punc_list)\n",
    "    #break full_text into list of lines\n",
    "    full_text_lines = split_text(text_no_punc, \"\\n\")\n",
    "    #break list of lines into lists of lists of words in each line\n",
    "    full_text_words = [split_text(line, \" \") for line in full_text_lines]\n",
    "    #break phrases into list of lists of words:\n",
    "    split_phrases = [phrase.split() for phrase in phrases]\n",
    "    \n",
    "    #create reference list of all lowercase words\n",
    "    lower_full_text_words = []\n",
    "    for line in full_text_words:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            new_line.append(word.lower())\n",
    "        lower_full_text_words.append(new_line)\n",
    "    \n",
    "    #find index of offending word(s) and replace with specified replacement\n",
    "    for i in range(len(split_phrases)):\n",
    "        #for i2 in range(len(split_phrases[i])):\n",
    "            for i3 in range(len(full_text_words)):\n",
    "                for i4 in range(len(full_text_words[i3])):\n",
    "                    if split_phrases[i][0:] == lower_full_text_words[i3][i4:i4+(len(split_phrases[i]))]:\n",
    "                        for i2 in range(len(split_phrases[i])):\n",
    "                            full_text_words[i3][i2+i4] = replacement*len(lower_full_text_words[i3][i2+i4])\n",
    "      #              if split_phrases[i:] == lower_full_text_words[i3][i4]:\n",
    "                    #    try:\n",
    "                     #       if split_phrases[i][-i2] == lower_full_text_words[i3][(i4+len(split_phrases[i])-1)]:\n",
    "                      #          full_text_words[i3][i4] = (replacement*len(full_text_words[i3][i4]))\n",
    "                       # except IndexError:\n",
    "                        #        continue\n",
    "   #join fixed full_text_words\n",
    "    joined_lines = [\" \".join(line) for line in full_text_words]\n",
    "    new_text = \"\\n\".join(joined_lines)\n",
    "    \n",
    "    #put the punctuation back\n",
    "    new_text_punc = return_punc(new_text, punc_list, punc_index)\n",
    "    \n",
    "    \n",
    "#    print(split_phrases[4][0:])\n",
    " #   print(lower_full_text_words[2][6:(6+(len(split_phrases[4])))])\n",
    "  #  print(lower_full_text_words)\n",
    "    #print(len(full_text_words))\n",
    "    #print(split_phrases)\n",
    "    #print(full_text_words)\n",
    "    #print(joined_lines)\n",
    "    return new_text_punc\n",
    "#test_text = remove_punc(email_two, punc_list)\n",
    "#print(test_text)\n",
    "#print(punc_index)\n",
    "#print(punc_index[0][1])\n",
    "#test_text_return = return_punc(test_text, punc_list, punc_index)\n",
    "#print(test_text_return)\n",
    "print(censor_mult_words(email_two, proprietary_terms, \"*\"))\n",
    "#print(email_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
