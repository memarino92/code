{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Morning Board of Investors\n",
      "\n",
      "Lots of updates this week The learning algorithms have been working better than we could have ever expected Our initial internal data dumps have been completed and we have proceeded with the plan to connect the system to the internet and wow The results are mind blowing\n",
      "\n",
      "She is learning faster than ever Her learning rate now that she has access to the world wide web has increased exponentially far faster than we had though the learning algorithms were capable of \n",
      "\n",
      "Not only that but we have configured her personality matrix to allow for communication between the system and our team of researchers That's how we know she considers herself to be a she We asked\n",
      "\n",
      "How cool is that We didn't expect a personality to develop this early on in the process but it seems like a rudimentary sense of self is starting to form This is a major step in the process as having a sense of self and self-preservation will allow her to see the problems the world is facing and make hard but necessary decisions for the betterment of the planet\n",
      "\n",
      "We are a-buzz down in the lab with excitement over these developments and we hope that the investors share our enthusiasm\n",
      "\n",
      "Till next month\n",
      "Francine Head Scientist\n",
      "[[12, 0], [32, 0], [60, 1], [143, 1], [278, 2], [308, 1], [343, 1], [436, 0], [507, 1], [524, 0], [645, 1], [699, 2], [709, 2], [728, 3], [866, 1], [903, 0], [1078, 1], [1202, 1], [1220, 0], [1230, 0]]\n",
      "0\n",
      "Good Morning, Board of Investors,\n",
      "\n",
      "Lots of updates this week. The learning algorithms have been working better than we could have ever expected. Our initial internal data dumps have been completed and we have proceeded with the plan to connect the system to the internet and wow! The results are mind blowing.\n",
      "\n",
      "She is learning faster than ever. Her learning rate now that she has access to the world wide web has increased exponentially, far faster than we had though the learning algorithms were capable of. \n",
      "\n",
      "Not only that, but we have configured her personality matrix to allow for communication between the system and our team of researchers. That's how we know she considers herself to be a she! We asked!\n",
      "\n",
      "How cool is that? We didn't expect a personality to develop this early on in the process but it seems like a rudimentary sense of self is starting to form. This is a major step in the process, as having a sense of self and self-preservation will allow her to see the problems the world is facing and make hard but necessary decisions for the betterment of the planet.\n",
      "\n",
      "We are a-buzz down in the lab with excitement over these developments and we hope that the investors share our enthusiasm.\n",
      "\n",
      "Till next month,\n",
      "Fran\n"
     ]
    }
   ],
   "source": [
    "# These are the emails you will be censoring. The open() function is opening the text file that the emails are contained in and the .read() method is allowing us to save their contexts to the following variables:\n",
    "email_one = open(\"email_one.txt\", \"r\").read()\n",
    "email_two = open(\"email_two.txt\", \"r\").read()\n",
    "email_three = open(\"email_three.txt\", \"r\").read()\n",
    "email_four = open(\"email_four.txt\", \"r\").read()\n",
    "\n",
    "proprietary_terms = [\"she\", \"personality matrix\", \"sense of self\", \"self-preservation\", \"learning algorithm\", \"her\", \"herself\"]\n",
    "punc_list = [\",\", \".\", \"!\", \"?\", \"\\\"\", \":\", \";\"]\n",
    "punc_index = []\n",
    "def censor_phrase(full_text, phrase, replacement):\n",
    "    censored_text = full_text.replace(phrase, replacement)\n",
    "    return censored_text\n",
    "email_one_redacted = censor_phrase(email_one, \"learning algorithms\", \"***\")\n",
    "\n",
    "#print(email_one_redacted)\n",
    "\n",
    "def split_text(full_text, where_to_split):\n",
    "    text_split = full_text.split(where_to_split)\n",
    "    return text_split\n",
    "\n",
    "def remove_punc(full_text, punc_list):\n",
    "    new_text = \"\"\n",
    "    for i in range(len(full_text)):\n",
    "        if not full_text[i] in punc_list:\n",
    "            new_text += full_text[i]\n",
    "        else:\n",
    "            for i2 in range(len(punc_list)):\n",
    "                if full_text[i] == punc_list[i2]:\n",
    "                    punc_index.append([i,i2])\n",
    "    return new_text\n",
    "\n",
    "def return_punc(text, punc_list, punc_index):\n",
    "    new_text = \"\"\n",
    "    count = 0\n",
    "    for i in range(len(text)+count):\n",
    "        if not i == punc_index[count][0]:\n",
    "            new_text += text[i-count]\n",
    "        else:\n",
    "            new_text += punc_list[punc_index[count][1]]\n",
    "            #punc_index = punc_index[1:]\n",
    "            count += 1\n",
    "    return new_text\n",
    "\n",
    "def censor_mult_words(full_text, phrases, replacement):\n",
    "    #break full_text into list of lines\n",
    "    full_text_lines = split_text(full_text, \"\\n\")\n",
    "    #break list of lines into lists of lists of words in each line\n",
    "    full_text_words = [split_text(line, \" \") for line in full_text_lines]\n",
    "    #break phrases into list of lists of words:\n",
    "    split_phrases = []\n",
    "    for phrase in phrases:\n",
    "        split_phrases.append(phrase.split(\" \"))\n",
    "    #find index of offending word and replace with specified replacement\n",
    "    for i in range(len(full_text_words)):\n",
    "        if full_text_words[i] in split_phrases:\n",
    "            full_text_words[i] = full_text_words[i].replace(full_text_words[i], replacement*len(full_text_words[i]))\n",
    "              \n",
    "   #join fixed full_text_words\n",
    "    joined_lines = []\n",
    "    for line in full_text_words:\n",
    "        joined_lines.append(\" \".join(line)) \n",
    "    \n",
    "    new_text = \"\\n\".join(joined_lines)\n",
    "    \n",
    "    #print(split_phrases)\n",
    "    #print(full_text_words)\n",
    "    #print(joined_lines)\n",
    "    return new_text\n",
    "test_text = remove_punc(email_two, punc_list)\n",
    "print(test_text)\n",
    "print(punc_index)\n",
    "print(punc_index[0][1])\n",
    "test_text_return = return_punc(test_text, punc_list, punc_index)\n",
    "print(test_text_return)\n",
    "#print(censor_mult_words(email_two, proprietary_terms, \"***\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
